{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from google.cloud.bigquery.client import Client\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "import math\n",
    "from decimal import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayP=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: The default value for dialect is changing to \"standard\" in a future version of pandas-gbq. Pass in dialect=\"legacy\" to disable this warning.\n"
     ]
    }
   ],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'G:/hedden/asu-msba-free-trial-conversion-credentials.json'\n",
    "bq_client = Client()\n",
    "client = bigquery.Client()\n",
    "projectID= 'infusionsoft-looker-poc'\n",
    "#standardSQLjob_config = bigquery.QueryJobConfig()\n",
    "##read Usage data from Bigquery, with the trial time all equal to 14 days\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "query_params = [\n",
    "    bigquery.ScalarQueryParameter('dayP', 'INT64', dayP)\n",
    "]\n",
    "job_config.query_parameters = query_params\n",
    "job_config.use_legacy_sql = False##Standard Sql\n",
    "query=\"\"\"SELECT\n",
    "  U.*,F.trial_date as trial_date\n",
    "FROM\n",
    "  `infusionsoft-looker-poc.asu_msba_free_trial_conversion.CONFIDENTIAL_usage_data` AS U JOIN\n",
    "  (SELECT app_name,trial_date,end_day FROM (SELECT app_name,CAST(trial_date AS date) as trial_date,\n",
    "  IF(CAST(trial_date AS date)<CAST(new_customer_date AS date),if(CAST(new_customer_date AS date)>DATE_ADD(CAST(trial_date AS date),INTERVAL @dayP DAY),DATE_ADD(CAST(trial_date AS date),INTERVAL @dayP DAY),CAST(new_customer_date AS date)),DATE_ADD(CAST(trial_date AS date),INTERVAL @dayP DAY)) as end_day\n",
    "  FROM `infusionsoft-looker-poc.asu_msba_free_trial_conversion.CONFIDENTIAL_free_trail_apps_table`) group by app_name,trial_date,end_day) AS F\n",
    "  ON U.appname=F.app_name\n",
    "WHERE\n",
    "  U.date>=F.trial_date\n",
    "  AND U.date<=F.end_day\n",
    "ORDER BY U.appname,U.date\"\"\"\n",
    "read= client.query(query,location='US',job_config=job_config)\n",
    "UsageData=read.result().to_dataframe()\n",
    "##make a copy of the USage data\n",
    "UD=UsageData.copy()\n",
    "DFreeTrial= pd.read_gbq('SELECT * FROM asu_msba_free_trial_conversion.CONFIDENTIAL_free_trail_apps_table ',projectID)\n",
    "DFree=DFreeTrial.copy()\n",
    "droplist=['tl471','wd410','mw416','kw563','ov450','jl500']\n",
    "DFree=DFree[~DFree['app_name'].isin(droplist)]\n",
    "UD=UD[~UD['appname'].isin(droplist)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oldcustomer(x):## get the old customer'Nl==1'\n",
    "    old=[]\n",
    "    for index,row in x.iterrows():\n",
    "        if pd.isna(row['new_customer_date']):\n",
    "            old.append(0)\n",
    "        elif row['new_customer_date']<row['trial_date']:\n",
    "            old.append(1)\n",
    "        else:\n",
    "            old.append(0)\n",
    "    x['Nl']=old\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase(x):## get the phase number\n",
    "    if pd.isna(x) or x=='None': return 0\n",
    "    else:\n",
    "        m=x.split(' ')[1]\n",
    "        return int(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropDup(df,cat): ## delete the duplicate caused by join\n",
    "    dfN = df.groupby(['app_name'])[[cat]].apply(lambda x: x.sort_values([cat], ascending=False).iloc[0, :])\n",
    "    return dfN\n",
    "def totaltar(df):##delete the duplicate caused by join and drop duplicates\n",
    "    df = df[['app_name', 'is_free_trial_initiated','trial_date', 'new_customer_date','first_contact_on', 'contact_lead_source', 'contact_phase', 'lead_lead_source','lead_converted_date']]\n",
    "    df=df.drop_duplicates()\n",
    "    AP = df[['app_name', 'is_free_trial_initiated', 'trial_date', 'new_customer_date']]\n",
    "    AP=AP.drop_duplicates()\n",
    "    Data = df.drop(['is_free_trial_initiated', 'trial_date'], axis=1)\n",
    "    DFL = []\n",
    "    for cat in ['first_contact_on', 'contact_lead_source', 'contact_phase', 'lead_lead_source', 'lead_converted_date']:\n",
    "        dfN = dropDup(Data,cat)\n",
    "        DFL.append(dfN)\n",
    "    DFreee = pd.concat(DFL, axis=1)\n",
    "    DFreee['app_name'] = DFreee.index\n",
    "    DFreee.reset_index(drop=True,inplace=True)\n",
    "    AP.drop_duplicates(inplace=True)\n",
    "    df = AP.merge(DFreee, left_on=['app_name'], right_on=['app_name'])\n",
    "    return df\n",
    "FR= totaltar(DFree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def soursetrans(text):## change the leadsource and contact source\n",
    "    sw = stopwords.words('english');text = text.lower();patten1 = string.digits;patten2 = string.punctuation\n",
    "    regex = re.compile(r\"[%s%s]\" % (patten1, patten2));text = regex.sub(\" \", text);regex = re.compile(r\"\\s+\")\n",
    "    text = regex.sub(\" \", text);textlist = text.split()\n",
    "    textclean = [w for w in textlist if w.lower() not in sw]\n",
    "    text1 = ['google', 'web', 'email', 'softwareadvice', 'forbes', 'doubleyoursales', 'www', 'com', 'emailfooter']\n",
    "    text2 = ['infusionsoft', 'freetrial', 'demo', 'direct', 'content', 'free', 'search']\n",
    "    text3 = ['partner', 'referral', 'recruiting', 'lead']\n",
    "    text4 = ['zunknown']\n",
    "    text5 = ['marketo', 'campaign']\n",
    "    text6 = ['salesline', 'mobile', 'onlinechat', 'offlinechat', 'chat', 'us', 'marketing', 'salesforce']\n",
    "    text7 = ['facebook', 'twitter', 'social']\n",
    "    output = [];retA = []\n",
    "    for x in textclean:\n",
    "        if x not in output:\n",
    "            output.append(x)\n",
    "    for i in output:\n",
    "        if i in text:\n",
    "            retA.append(i)\n",
    "    if retA == []:\n",
    "        retA = output\n",
    "    group = 'other'\n",
    "    for i in retA:\n",
    "        if i in text1: group = 'advertise'\n",
    "        if i in text2: group = 'directsearch'\n",
    "        if i in text3: group = 'refer'\n",
    "        if i in text4: group = 'unknown'\n",
    "        if i in text5: group = 'marketo_campaign'\n",
    "        if i in text6: group = 'chat'\n",
    "        if i in text7: group = 'socialmedia'\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline(x):\n",
    "    if x<=-13 or x==None: return -14\n",
    "    else:return x\n",
    "def floatc(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return float(x)\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Free_process(df1):#process \n",
    "    df1.loc[:,'contact_phase']=df1['contact_phase'].apply(phase)\n",
    "    df1[['contact_lead_source','lead_lead_source']]=df1[['contact_lead_source','lead_lead_source']].fillna('zunknown')\n",
    "    for cate in ['contact_lead_source','lead_lead_source']:\n",
    "        df1.loc[:,cate] = df1[cate].apply(lambda x: soursetrans(x))\n",
    "    Tnow = datetime.now()\n",
    "    for date in ['first_contact_on','lead_converted_date']:\n",
    "        df1.loc[:,date] = pd.to_datetime(df1[date])\n",
    "        df1.loc[:,date].fillna(Tnow, inplace=True)\n",
    "        df1.loc[:,date] = ((pd.to_datetime(df1['trial_date'].dt.date) - pd.to_datetime(df1[date].dt.date)) / np.timedelta64(1, 'D')).astype('int')\n",
    "        df1.loc[:,date]=df1[date].apply(outline)\n",
    "    df1.loc[:,'is_free_trial_initiated']=df1['is_free_trial_initiated'].apply(lambda x:x+0)\n",
    "    df1=df1[df1['is_free_trial_initiated']==1]\n",
    "    df1.drop(['is_free_trial_initiated'],inplace=True,axis=1)\n",
    "    df1['trial_date']=pd.to_datetime(df1['trial_date'].dt.date)\n",
    "    df1=oldcustomer(df1)\n",
    "    df1=df1[df1['Nl']==0]\n",
    "    df1.loc[:,'trial_date']=pd.to_datetime(df1['trial_date'].dt.date)\n",
    "    df1=df1.drop_duplicates()\n",
    "    df1.reset_index(drop=True, inplace=True)\n",
    "    df1.drop(['Nl'],axis=1,inplace=True)\n",
    "    df1['contact_phase'] = df1['contact_phase'].astype('int64')\n",
    "    df1['target'] = df1['new_customer_date'].apply(lambda x:0 if pd.isna(x) else 1 )\n",
    "    numb=[x for x in df1.columns.tolist() if x not in ['app_name','trial_date','new_customer_date','account_id','contact_id','lead_lead_source','contact_lead_source']]\n",
    "    MMSF = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    df1[numb] = pd.DataFrame(MMSF.fit_transform(df1[numb]), index=df1.index)\n",
    "    ohe = preprocessing.OneHotEncoder(sparse=False, dtype=int, handle_unknown='ignore')\n",
    "    cat = ['lead_lead_source','contact_lead_source']\n",
    "    Xcat = pd.DataFrame(ohe.fit_transform(df1[cat]), columns=ohe.get_feature_names(), index=df1.index)\n",
    "    dfN= pd.concat([df1, Xcat], axis=1)\n",
    "    dff=dfN[[x for x in dfN.columns.tolist() if x not in cat]]\n",
    "    return dff,MMSF,ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "NNN=FR.copy()\n",
    "Freee,MMSF,OHE=Free_process(NNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD1=UD.drop(['NUM_CONTACTS'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "## preprocess and minmax scale the USage data\n",
    "def Usage_process(df2):\n",
    "    df2=df2.drop_duplicates()\n",
    "    df2.loc[:,'free_email']=df2['free_email'].apply(lambda x:x+0)\n",
    "    df2.loc[:,'invoice_amount'] = df2['invoice_amount'].apply(floatc)\n",
    "    df2=df2.fillna(0)\n",
    "    df2.loc[:,'trial_date']=pd.to_datetime(df2['trial_date'])\n",
    "    MMSdf = preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "    numb2 = [x for x in df2.columns.tolist() if x not in ['appname', 'trial_date', 'date']]\n",
    "    df2[numb2] = pd.DataFrame(MMSdf.fit_transform(df2[numb2]), index=df2.index)\n",
    "    return df2,MMSdf\n",
    "UD2,MMU=Usage_process(UD1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usedat(UD2,dayP):\n",
    "    ZeroN=np.zeros((14,72))\n",
    "    Usage={}\n",
    "    for name,group in UD2.groupby(['appname','trial_date']):\n",
    "        G=group[[x for x in group.columns.tolist() if x not in ['appname','trial_date']]].groupby(['date']).agg('mean')\n",
    "        L=G.shape[0]\n",
    "        New=np.concatenate((G.values, ZeroN), axis=0)\n",
    "        TG=New[0:dayP]\n",
    "        Usage[name]=[name,TG,L]\n",
    "    Usagedata=pd.DataFrame.from_dict(Usage, orient='index',columns=['name','Gdata','len'])\n",
    "    return Usagedata\n",
    "Usagedata=get_usedat(UD2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Free={}\n",
    "for name,group in Freee.groupby(['app_name','trial_date']):\n",
    "    if name not in Usagedata['name'].values.tolist():\n",
    "        continue\n",
    "    target=int(group['target'].values[0])\n",
    "    K=group[[x for x in group.columns.tolist() if x not in ['app_name','trial_date','target','is_free_trial_initiated','new_customer_date']]].values\n",
    "    Len=group.shape[0]\n",
    "    Us=Usagedata[Usagedata['name']==name]['Gdata'].values[0]\n",
    "    Ul=Usagedata[Usagedata['name']==name]['len'].values[0]\n",
    "    Free[name]=[name,K,target,Len,Us,Ul]\n",
    "Freedata=pd.DataFrame.from_dict(Free, orient='index',columns=['name','Fdata','target','Flen','Usage','Ul'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1420"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Freedata['target'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    Con=df[df['target']==1]\n",
    "    Ncon=df[df['target']==0]\n",
    "    TRc=Con.sample(frac=0.9)\n",
    "    TSc=Con[~Con.index.isin(TRc.index)]\n",
    "    TRn=Ncon.sample(frac=0.2)\n",
    "    TSn=Ncon[~Ncon.index.isin(TRn.index)]\n",
    "    Train=pd.concat([TRc.sample(frac=2.5,replace=True),TRn])\n",
    "    Test=pd.concat([TSc.sample(frac=2.5,replace=True),TSn])\n",
    "    Train['target']=Train['target'].apply(lambda x:[0,1] if x==1 else [1,0])\n",
    "    Test['target']=Test['target'].apply(lambda x:[0,1] if x==1 else [1,0])\n",
    "    Test=pd.concat([TSc.sample(frac=2.5,replace=True),TSn])\n",
    "    Train=Train.sort_values(by=['Ul'],ascending=False)\n",
    "    Test=Test.sort_values(by=['Ul'],ascending=False)\n",
    "    Train_x=np.array(Train['Usage'].tolist())\n",
    "    Train_y=np.array(Train['target'].tolist()).reshape(Train['target'].shape[0],-1)\n",
    "    Train_S=Train['Ul'].tolist()\n",
    "    Test_x=np.array(Test['Usage'].tolist())\n",
    "    Test_y=np.array(Test['target'].tolist()).reshape(Test['target'].shape[0],-1)\n",
    "    Test_S=Test['Ul'].tolist()        \n",
    "    return Train_x,Train_y,Train_S,Test_x,Test_y,Test_S\n",
    "Train_x,Train_y,Train_S,Test_x,Test_y,Test_S=split(Freedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01    # 学习率\n",
    "n_steps = dayP            # LSTM 展开步数（时序持续长度）\n",
    "n_inputs = 72           # 输入节点数\n",
    "n_hiddens = 64         # 隐层节点数\n",
    "n_layers = 2            # LSTM layer 层数\n",
    "n_classes = 2          # 输出节点数（分类数目）\n",
    "_batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor placeholder\n",
    "with tf.name_scope('inputs'):\n",
    "    x = tf.placeholder(tf.float32, [None, n_steps, n_inputs], name='x_input')     # input\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes], name='y_input')               # output\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob_input')           # keep how not dropout\n",
    "    batch_size = tf.placeholder(tf.int32, [], name='batch_size_input')\n",
    "    x_lengths = [0]*_batch_size\n",
    "# weights and biases\n",
    "with tf.name_scope('weights'):\n",
    "    Weights = tf.Variable(tf.truncated_normal([n_hiddens, n_classes],stddev=0.1), dtype=tf.float32, name='W')\n",
    "    tf.summary.histogram('output_layer_weights', Weights)\n",
    "with tf.name_scope('biases'):\n",
    "    biases = tf.Variable(tf.random_normal([n_classes]), name='b')\n",
    "    tf.summary.histogram('output_layer_biases', biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN structure\n",
    "def RNN_LSTM(x,x_lengths,Weights, biases,):\n",
    "    x = tf.reshape(x, [-1, n_steps, n_inputs])\n",
    "    # LSTM cell\n",
    "    # cell dropout\n",
    "    def attn_cell():\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hiddens,state_is_tuple=True)\n",
    "        with tf.name_scope('lstm_dropout'):\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "    # attn_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "    # [attn_cell() for _ in range(n_layers)]\n",
    "    enc_cells = []\n",
    "    for i in range(0, n_layers):\n",
    "        enc_cells.append(attn_cell())\n",
    "    with tf.name_scope('lstm_cells_layers'):\n",
    "        mlstm_cell = tf.contrib.rnn.MultiRNNCell(enc_cells, state_is_tuple=True)\n",
    "    # 0 state\n",
    "    _init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    # dynamic_rnn\n",
    "    outputs, states = tf.nn.dynamic_rnn(mlstm_cell, x, sequence_length=x_lengths,initial_state=_init_state, dtype=tf.float32, time_major=False)\n",
    "    #return tf.matmul(outputs[:,-1,:], Weights) + biases\n",
    "    return tf.nn.softmax(tf.matmul(outputs[:,-1,:], Weights) + biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-2934260ffa4f>:7: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('output_layer'):\n",
    "    pred = RNN_LSTM(x, x_lengths, Weights, biases)\n",
    "    tf.summary.histogram('outputs', pred)\n",
    "# cost\n",
    "with tf.name_scope('loss'):\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred),reduction_indices=[1]))\n",
    "    tf.summary.scalar('loss', cost)\n",
    "# optimizer\n",
    "with tf.name_scope('train'):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "# accuarcy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.metrics.accuracy(labels=tf.argmax(y, axis=1), predictions=tf.argmax(pred, axis=1))[1]\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [Expected shape for Tensor output_layer/rnn/sequence_length:0 is ] [118] [ but saw shape: ] [128]\n\t [[node output_layer/rnn/Assert/Assert (defined at <ipython-input-34-2934260ffa4f>:20)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](output_layer/rnn/All, output_layer/rnn/Assert/Assert/data_0, output_layer/rnn/stack, output_layer/rnn/Assert/Assert/data_2, output_layer/rnn/Shape_1)]]\n\nCaused by op 'output_layer/rnn/Assert/Assert', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-35-ebc177e244c7>\", line 2, in <module>\n    pred = RNN_LSTM(x, x_lengths, Weights, biases)\n  File \"<ipython-input-34-2934260ffa4f>\", line 20, in RNN_LSTM\n    outputs, states = tf.nn.dynamic_rnn(mlstm_cell, x, sequence_length=x_lengths,initial_state=_init_state, dtype=tf.float32, time_major=False)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 651, in dynamic_rnn\n    [_assert_has_shape(sequence_length, [batch_size])]):\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 646, in _assert_has_shape\n    packed_shape, \" but saw shape: \", x_shape])\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 189, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 159, in Assert\n    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 52, in _assert\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor output_layer/rnn/sequence_length:0 is ] [118] [ but saw shape: ] [128]\n\t [[node output_layer/rnn/Assert/Assert (defined at <ipython-input-34-2934260ffa4f>:20)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](output_layer/rnn/All, output_layer/rnn/Assert/Assert/data_0, output_layer/rnn/stack, output_layer/rnn/Assert/Assert/data_2, output_layer/rnn/Shape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: assertion failed: [Expected shape for Tensor output_layer/rnn/sequence_length:0 is ] [118] [ but saw shape: ] [128]\n\t [[{{node output_layer/rnn/Assert/Assert}} = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](output_layer/rnn/All, output_layer/rnn/Assert/Assert/data_0, output_layer/rnn/stack, output_layer/rnn/Assert/Assert/data_2, output_layer/rnn/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-3d6fd6b3bf3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNbatch\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mTrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_S\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: assertion failed: [Expected shape for Tensor output_layer/rnn/sequence_length:0 is ] [118] [ but saw shape: ] [128]\n\t [[node output_layer/rnn/Assert/Assert (defined at <ipython-input-34-2934260ffa4f>:20)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](output_layer/rnn/All, output_layer/rnn/Assert/Assert/data_0, output_layer/rnn/stack, output_layer/rnn/Assert/Assert/data_2, output_layer/rnn/Shape_1)]]\n\nCaused by op 'output_layer/rnn/Assert/Assert', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-35-ebc177e244c7>\", line 2, in <module>\n    pred = RNN_LSTM(x, x_lengths, Weights, biases)\n  File \"<ipython-input-34-2934260ffa4f>\", line 20, in RNN_LSTM\n    outputs, states = tf.nn.dynamic_rnn(mlstm_cell, x, sequence_length=x_lengths,initial_state=_init_state, dtype=tf.float32, time_major=False)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 651, in dynamic_rnn\n    [_assert_has_shape(sequence_length, [batch_size])]):\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 646, in _assert_has_shape\n    packed_shape, \" but saw shape: \", x_shape])\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 189, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 159, in Assert\n    return gen_logging_ops._assert(condition, data, summarize, name=\"Assert\")\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 52, in _assert\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor output_layer/rnn/sequence_length:0 is ] [118] [ but saw shape: ] [128]\n\t [[node output_layer/rnn/Assert/Assert (defined at <ipython-input-34-2934260ffa4f>:20)  = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](output_layer/rnn/All, output_layer/rnn/Assert/Assert/data_0, output_layer/rnn/stack, output_layer/rnn/Assert/Assert/data_2, output_layer/rnn/Shape_1)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # training\n",
    "    step = 1\n",
    "    Nbatch=math.ceil(int(Train_x.shape[0])/_batch_size)\n",
    "    for i in range(Nbatch ):\n",
    "        batch_x, batch_y,Sequence= Train_x[i*_batch_size:(i+1)*_batch_size,],Train_y[i*_batch_size:(i+1)*_batch_size,],Train_S[i*_batch_size:(i+1)*_batch_size]\n",
    "        sess.run(train_op, feed_dict={x:batch_x, y:batch_y, keep_prob:0.5, batch_size:_batch_size})\n",
    "        if (i + 1) % 100 == 0:\n",
    "            loss = sess.run(cost, feed_dict={x:batch_x, y:batch_y,x_lengths:Sequence, keep_prob:1.0, batch_size:_batch_size})\n",
    "            acc = sess.run(accuracy, feed_dict={x:batch_x, y:batch_y, keep_prob:1.0, batch_size:_batch_size})\n",
    "            print('Iter: %d' % ((i+1) * _batch_size), '| train loss: %.6f' % loss, '| train accuracy: %.6f' % acc)\n",
    "            train_result = sess.run(merged, feed_dict={x:batch_x, y:batch_y, keep_prob:1.0, batch_size:_batch_size})\n",
    "            test_result = sess.run(merged, feed_dict={x:test_x, y:test_y, keep_prob:1.0, batch_size:test_x.shape[0]})\n",
    "            train_writer.add_summary(train_result,i+1)\n",
    "            test_writer.add_summary(test_result,i+1)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    # prediction\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x:test_x, y:test_y, keep_prob:1.0, batch_size:test_x.shape[0]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class LSTMRNN(object):\n",
    "    def __init__(self, n_steps, input_size, output_size, cell_size, batch_size,Seq_length):\n",
    "        self.seq=Seq_length\n",
    "        self.n_steps = n_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.cell_size = cell_size\n",
    "        self.batch_size = batch_size\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.xs = tf.placeholder(tf.float32, [None, n_steps, input_size], name='xs')\n",
    "            self.ys = tf.placeholder(tf.float32, [None, n_steps, output_size], name='ys')\n",
    "        with tf.variable_scope('in_hidden'):\n",
    "            self.add_input_layer()\n",
    "        with tf.variable_scope('LSTM_cell'):\n",
    "            self.add_cell()\n",
    "        with tf.variable_scope('out_hidden'):\n",
    "            self.add_output_layer()\n",
    "        with tf.name_scope('cost'):\n",
    "            self.compute_cost()\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(LR).minimize(self.cost)\n",
    "\n",
    "    def add_input_layer(self,):\n",
    "        Ws_in = self._weight_variable([self.input_size, self.cell_size])\n",
    "        # bs (cell_size, )\n",
    "        bs_in = self._bias_variable([self.cell_size,])\n",
    "        # l_in_y = (batch * n_steps, cell_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            l_in_y = tf.matmul(l_in_x, Ws_in) + bs_in\n",
    "        # reshape l_in_y ==> (batch, n_steps, cell_size)\n",
    "        self.l_in_y = tf.reshape(l_in_y, [-1, self.n_steps, self.cell_size], name='2_3D')\n",
    "\n",
    "    def add_cell(self):\n",
    "        lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.cell_size, forget_bias=1.0, state_is_tuple=True)\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.cell_init_state = lstm_cell.zero_state(self.batch_size, dtype=tf.float32)\n",
    "        self.cell_outputs, self.cell_final_state = tf.nn.dynamic_rnn(\n",
    "            lstm_cell, self.l_in_y, initial_state=self.cell_init_state, time_major=False)\n",
    "\n",
    "    def add_output_layer(self):\n",
    "        # shape = (batch * steps, cell_size)\n",
    "        l_out_x = tf.reshape(self.cell_outputs, [-1, self.cell_size], name='2_2D')\n",
    "        Ws_out = self._weight_variable([self.cell_size, self.output_size])\n",
    "        bs_out = self._bias_variable([self.output_size, ])\n",
    "        # shape = (batch * steps, output_size)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            self.pred = tf.matmul(l_out_x, Ws_out) + bs_out\n",
    "\n",
    "    def compute_cost(self):\n",
    "        losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [tf.reshape(self.pred, [-1], name='reshape_pred')],\n",
    "            [tf.reshape(self.ys, [-1], name='reshape_target')],\n",
    "            [tf.ones([self.batch_size * self.n_steps], dtype=tf.float32)],\n",
    "            average_across_timesteps=True,\n",
    "            softmax_loss_function=self.ms_error,\n",
    "            name='losses'\n",
    "        )\n",
    "        with tf.name_scope('average_cost'):\n",
    "            self.cost = tf.div(\n",
    "                tf.reduce_sum(losses, name='losses_sum'),\n",
    "                self.batch_size,\n",
    "                name='average_cost')\n",
    "            tf.summary.scalar('cost', self.cost)\n",
    "\n",
    "    @staticmethod\n",
    "    def ms_error(labels, logits):\n",
    "        return tf.square(tf.subtract(labels, logits))\n",
    "\n",
    "    def _weight_variable(self, shape, name='weights'):\n",
    "        initializer = tf.random_normal_initializer(mean=0., stddev=1.,)\n",
    "        return tf.get_variable(shape=shape, initializer=initializer, name=name)\n",
    "\n",
    "    def _bias_variable(self, shape, name='biases'):\n",
    "        initializer = tf.constant_initializer(0.1)\n",
    "        return tf.get_variable(name=name, shape=shape, initializer=initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##def split(df):\n",
    "    Con=df[df['target']==1]\n",
    "    Ncon=df[df['target']==0]\n",
    "    TRc=Con.sample(frac=0.9)\n",
    "    TSc=Con[~Con.index.isin(TRc.index)]\n",
    "    TRn=Ncon.sample(frac=0.2)\n",
    "    TSn=Ncon[~Ncon.index.isin(TRn.index)]\n",
    "    Train=pd.concat([TRc.sample(frac=2.5,replace=True),TRn])\n",
    "    Test=pd.concat([TSc.sample(frac=2.5,replace=True),TSn])\n",
    "    Train=Train.sort_values(by=['Ul'],ascending=False)\n",
    "    Test=Test.sort_values(by=['Ul'],ascending=False)\n",
    "    Train_x=torch.from_numpy(np.array(Train['Usage'].tolist())).double()\n",
    "    Train_y=torch.from_numpy(np.array(Train['target'].tolist()).reshape(Train['target'].shape[0],-1)).double()\n",
    "    Train_S=Train['Ul'].tolist()\n",
    "    Test_x=torch.from_numpy(np.array(Test['Usage'].tolist())).double()\n",
    "    Test_y=torch.from_numpy(np.array(Test['target'].tolist()).reshape(Test['target'].shape[0],-1)).double()\n",
    "    Test_S=Test['Ul'].tolist()          \n",
    "    return Train_x,Train_y,Train_S,Test_x,Test_y,Test_S\n",
    "Train_x,Train_y,Train_S,Test_x,Test_y,Test_S=split(Freedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPOCH=3\n",
    "BATCH_SIZE=10\n",
    "TIME_STEP=14\n",
    "INPUT_SIZE=72\n",
    "LR=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_layers=3,nb_lstm_units=80,input_size=72,batch_size=10):\n",
    "        super(RNN,self).__init__()\n",
    "        self.nb_lstm_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.inputsize = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_tags=2\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.inputsize,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units,2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_b = torch.randn(self.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X,X_length):\n",
    "        X=X.type(torch.float32)\n",
    "        self.hidden = self.init_hidden()\n",
    "        X=Variable(X)\n",
    "        X=torch.nn.utils.rnn.pack_padded_sequence(X, X_length, batch_first=True)\n",
    "        X,self.hidden= self.lstm(X, self.hidden)\n",
    "        X, _= torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "        X = self.hidden_to_tag(X)\n",
    "        X = F.log_softmax(X, dim=1)\n",
    "        X = X.view(batch_size, seq_len, self.nb_tags)\n",
    "        Y_hat = X\n",
    "        return Y_hat\n",
    "rnn = RNN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer=torch.optim.Adam(rnn.parameters(),lr=LR)\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "\n",
    "n_batch=math.ceil(Train_x.shape[0]/BATCH_SIZE)\n",
    "for epoch in range(1):\n",
    "    for i in range(n_batch):\n",
    "        local_X, local_y,Sequence= Train_x[i*10:(i+1)*10,],Train_y[i*10:(i+1)*10,],Train_S[i*10:(i+1)*10]\n",
    "        output=rnn(local_X,X_length=Sequence)\n",
    "        loss=loss_func(output,local_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 50 == 0:\n",
    "            test_output = rnn(Test_x,Seq=Test_S)                   # (samples, time_step, input_size)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.numpy()\n",
    "            accuracy = float((pred_y == Test_y).astype(int).sum()) / float(Test_y.size)\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
